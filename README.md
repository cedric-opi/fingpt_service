# FinGPT Stock Forecaster Server

Optimized for **Mac Mini M4 (16GB RAM)**

Financial stock forecasting using **FinGPT (Llama-2-7B)** with **4-bit quantization** for blazing-fast inference.

---

## ğŸ¯ Key Features

* âš¡ **15â€“20x faster** than baseline (12â€“18 min vs 3â€“5 hours)
* ğŸ’¾ **60% less memory** (4â€“6GB vs 14â€“16GB)
* ğŸš€ **4-bit quantization** with bitsandbytes
* ğŸ’° Intelligent caching for repeated queries
* ğŸ—ï¸ Clean modular architecture
* ğŸ“Š Real-time streaming or batch responses
* ğŸ”§ Easy configuration with a single config file

---

# ğŸ› ï¸ Installation & Execution Guide

## ğŸ macOS Installation (Apple Silicon Recommended)

### 1. Clone Project

```
git clone <repo-url>
cd fingpt-server
```

### 2. Create Virtual Environment

```
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```
pip install -r requirements.txt
```

### 4. Install llama.cpp with Metal Acceleration

```
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
python -c "from llama_cpp import Llama; print('âœ… llama-cpp-python installed!')"
```

### 5. Download GGUF Model

```
pip install huggingface-hub
mkdir -p models
huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF \
  llama-2-7b-chat.Q4_K_M.gguf \
  --local-dir ./models \
  --local-dir-use-symlinks False
```

If required:

```
huggingface-cli login
```

### 6. Start Server

```
python server.py
```

---

## ğŸªŸ Windows Installation

### 1. Clone Project

```
git clone <repo-url>
cd fingpt-server
```

### 2. Create Virtual Environment

```
python -m venv venv
.\venv\Scripts\activate
```

### 3. Install Dependencies

```
pip install -r requirements.txt
```

### 4. Install CPU-Only llama.cpp Wheel

```
pip install llama-cpp-python --no-cache-dir --force-reinstall
python -c "from llama_cpp import Llama; print('âœ… llama-cpp-python installed on Windows!')"
```

### 5. Download Model

```
pip install huggingface-hub
mkdir models
huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF \
  llama-2-7b-chat.Q4_K_M.gguf \
  --local-dir ./models \
  --local-dir-use-symlinks False
```

If required:

```
huggingface-cli login
```

### 6. Start Server

```
python server.py
```

---

# ğŸ”§ Configuration

### 1. Create `.env` File

```
cp .env.example .env
```

Add Finnhub API Key:

```
FINNHUB_API_KEY=your_key_here
```

### 2. Modify `config.py`

Best speed (recommended):

```
USE_4BIT_QUANTIZATION = True
```

Fallback (if bitsandbytes fails):

```
USE_4BIT_QUANTIZATION = False
```

---

# â–¶ï¸ Run Server

```
python server.py
```

---

# ğŸ§ª Testing

### Health Check

```
curl http://localhost:8000/health
```

### Generate Forecast

```
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "ticker": "AAPL",
    "end_date": "2024-10-30",
    "stream": false
  }'
```

---

# ğŸ“ Project Structure

```
fingpt-server/
â”œâ”€â”€ config.py
â”œâ”€â”€ model.py
â”œâ”€â”€ data_service.py
â”œâ”€â”€ cache.py
â”œâ”€â”€ server_optimized.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ benchmark.py
â”‚   â””â”€â”€ test_cache.sh
â”‚
â””â”€â”€ docs/
```

---

# ğŸ“¡ API Endpoints

### **POST /v1/chat/completions** â€” Generate Forecast

```
{
  "ticker": "AAPL",
  "end_date": "2024-10-30",
  "past_weeks": 4,
  "include_financials": false,
  "temperature": 0.2,
  "max_new_tokens": 256,
  "stream": true
}
```

### **GET /health** â€” Health Check

### **GET /cache/stats** â€” Cache Statistics

### **GET /debug/model** â€” Model Info

---

# ğŸ› Troubleshooting

### bitsandbytes Fails

```
USE_4BIT_QUANTIZATION = False
USE_8BIT_QUANTIZATION = False
```

### Out of Memory

```
USE_4BIT_QUANTIZATION = True
DEFAULT_MAX_NEW_TOKENS = 200
LOW_MEMORY_MODE = True
```

### Slow Generation

* Ensure quantization enabled
* Reduce max_new_tokens
* Run benchmark:

```
python benchmark.py
```

### Cache Not Working

```
./scripts/test_cache.sh
```

---

# ğŸ”’ Environment Variables

```
FINNHUB_API_KEY=your_key_here
HF_TOKEN=your_huggingface_token
```

---

# ğŸš€ Performance Tips

* Use 4-bit quantization
* Reduce max_new_tokens for speed
* Enable caching
* Free system memory
* Use low temperature (0.1â€“0.2)

---
